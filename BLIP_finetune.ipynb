{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "399fe1e2b7e94d32bb53feb33122974c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba5ed68d4cc54a24b36820770c1d8033",
              "IPY_MODEL_914ff72736ac419094d3301f40db98f1",
              "IPY_MODEL_d9b9a8956fcc47aeaabd539a3f4ac420"
            ],
            "layout": "IPY_MODEL_0782e48104614decaf3a4ba31a4ed1c2"
          }
        },
        "ba5ed68d4cc54a24b36820770c1d8033": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd5a85a709204ddca8dc6179321e0de6",
            "placeholder": "​",
            "style": "IPY_MODEL_81ac2128a3ee448882c4988ec2e60175",
            "value": "Map: 100%"
          }
        },
        "914ff72736ac419094d3301f40db98f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_307c96240fca426aae05fbe653891d4d",
            "max": 9949,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e20d06086ca4f7bad53a1dbc6c80130",
            "value": 9949
          }
        },
        "d9b9a8956fcc47aeaabd539a3f4ac420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29a9882e9de0445592d27e7510c7d5d4",
            "placeholder": "​",
            "style": "IPY_MODEL_00a4c9276a434adfab025cb3014346cc",
            "value": " 9949/9949 [09:55&lt;00:00, 18.65 examples/s]"
          }
        },
        "0782e48104614decaf3a4ba31a4ed1c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd5a85a709204ddca8dc6179321e0de6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81ac2128a3ee448882c4988ec2e60175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "307c96240fca426aae05fbe653891d4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e20d06086ca4f7bad53a1dbc6c80130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29a9882e9de0445592d27e7510c7d5d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00a4c9276a434adfab025cb3014346cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmUIYbcD8a9U",
        "outputId": "63ebeae5-c997-4f88-b95b-7707798104c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=143dcfe5641543fd876837870e47aa0b6af0f2bd3a60aba15dbb8f7377b4f0fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: xxhash, dill, rouge_score, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-3.0.1 dill-0.3.8 evaluate-0.4.3 multiprocess-0.70.16 rouge_score-0.1.2 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch torchvision pillow tqdm evaluate rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "import evaluate\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from transformers import DataCollatorForSeq2Seq"
      ],
      "metadata": {
        "id": "klZoSGn_Lcx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Configuration Parameters\n",
        "# -------------------------------\n",
        "DATASET_NAME = \"tomytjandra/h-and-m-fashion-caption-12k\"  # Replace with your dataset name or path\n",
        "MODEL_NAME = \"Salesforce/blip-image-captioning-base\"  # Pre-trained BLIP model\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/BLIP\"  # Directory to save the fine-tuned model\n",
        "BATCH_SIZE = 8  # Adjust based on your GPU memory\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 5e-5\n",
        "MAX_LENGTH = 224  # Maximum length of generated captions\n",
        "SEED = 42\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "TEST_SPLIT_PERCENTAGE = 20\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "if USE_GPU:\n",
        "    torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "id": "lOpWtvwWLia4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Load the Dataset\n",
        "# -------------------------------\n",
        "# Replace 'your_dataset_name' with your actual dataset name or local path\n",
        "dataset = load_dataset(DATASET_NAME)\n",
        "\n",
        "# -------------------------------\n",
        "# Load BLIP Model and Processor\n",
        "# -------------------------------\n",
        "processor = BlipProcessor.from_pretrained(MODEL_NAME)\n",
        "model = BlipForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if USE_GPU else \"cpu\")\n",
        "# Freeze the encoder\n",
        "# Freeze the image encoder\n",
        "image_encoder = model.vision_model\n",
        "for param in image_encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "UlEZmq48L6nD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8567a0e9-da90-42b3-f0e9-f0eb6e6252a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embeddings.class_embedding: requires_grad=False\n",
            "embeddings.position_embedding: requires_grad=False\n",
            "embeddings.patch_embedding.weight: requires_grad=False\n",
            "embeddings.patch_embedding.bias: requires_grad=False\n",
            "encoder.layers.0.self_attn.qkv.weight: requires_grad=False\n",
            "encoder.layers.0.self_attn.qkv.bias: requires_grad=False\n",
            "encoder.layers.0.self_attn.projection.weight: requires_grad=False\n",
            "encoder.layers.0.self_attn.projection.bias: requires_grad=False\n",
            "encoder.layers.0.layer_norm1.weight: requires_grad=False\n",
            "encoder.layers.0.layer_norm1.bias: requires_grad=False\n",
            "encoder.layers.0.mlp.fc1.weight: requires_grad=False\n",
            "encoder.layers.0.mlp.fc1.bias: requires_grad=False\n",
            "encoder.layers.0.mlp.fc2.weight: requires_grad=False\n",
            "encoder.layers.0.mlp.fc2.bias: requires_grad=False\n",
            "encoder.layers.0.layer_norm2.weight: requires_grad=False\n",
            "encoder.layers.0.layer_norm2.bias: requires_grad=False\n",
            "encoder.layers.1.self_attn.qkv.weight: requires_grad=False\n",
            "encoder.layers.1.self_attn.qkv.bias: requires_grad=False\n",
            "encoder.layers.1.self_attn.projection.weight: requires_grad=False\n",
            "encoder.layers.1.self_attn.projection.bias: requires_grad=False\n",
            "encoder.layers.1.layer_norm1.weight: requires_grad=False\n",
            "encoder.layers.1.layer_norm1.bias: requires_grad=False\n",
            "encoder.layers.1.mlp.fc1.weight: requires_grad=False\n",
            "encoder.layers.1.mlp.fc1.bias: requires_grad=False\n",
            "encoder.layers.1.mlp.fc2.weight: requires_grad=False\n",
            "encoder.layers.1.mlp.fc2.bias: requires_grad=False\n",
            "encoder.layers.1.layer_norm2.weight: requires_grad=False\n",
            "encoder.layers.1.layer_norm2.bias: requires_grad=False\n",
            "encoder.layers.2.self_attn.qkv.weight: requires_grad=False\n",
            "encoder.layers.2.self_attn.qkv.bias: requires_grad=False\n",
            "encoder.layers.2.self_attn.projection.weight: requires_grad=False\n",
            "encoder.layers.2.self_attn.projection.bias: requires_grad=False\n",
            "encoder.layers.2.layer_norm1.weight: requires_grad=False\n",
            "encoder.layers.2.layer_norm1.bias: requires_grad=False\n",
            "encoder.layers.2.mlp.fc1.weight: requires_grad=False\n",
            "encoder.layers.2.mlp.fc1.bias: requires_grad=False\n",
            "encoder.layers.2.mlp.fc2.weight: requires_grad=False\n",
            "encoder.layers.2.mlp.fc2.bias: requires_grad=False\n",
            "encoder.layers.2.layer_norm2.weight: requires_grad=False\n",
            "encoder.layers.2.layer_norm2.bias: requires_grad=False\n",
            "encoder.layers.3.self_attn.qkv.weight: requires_grad=False\n",
            "encoder.layers.3.self_attn.qkv.bias: requires_grad=False\n",
            "encoder.layers.3.self_attn.projection.weight: requires_grad=False\n",
            "encoder.layers.3.self_attn.projection.bias: requires_grad=False\n",
            "encoder.layers.3.layer_norm1.weight: requires_grad=False\n",
            "encoder.layers.3.layer_norm1.bias: requires_grad=False\n",
            "encoder.layers.3.mlp.fc1.weight: requires_grad=False\n",
            "encoder.layers.3.mlp.fc1.bias: requires_grad=False\n",
            "encoder.layers.3.mlp.fc2.weight: requires_grad=False\n",
            "encoder.layers.3.mlp.fc2.bias: requires_grad=False\n",
            "encoder.layers.3.layer_norm2.weight: requires_grad=False\n",
            "encoder.layers.3.layer_norm2.bias: requires_grad=False\n",
            "encoder.layers.4.self_attn.qkv.weight: requires_grad=False\n",
            "encoder.layers.4.self_attn.qkv.bias: requires_grad=False\n",
            "encoder.layers.4.self_attn.projection.weight: requires_grad=False\n",
            "encoder.layers.4.self_attn.projection.bias: requires_grad=False\n",
            "encoder.layers.4.layer_norm1.weight: requires_grad=False\n",
            "encoder.layers.4.layer_norm1.bias: requires_grad=False\n",
            "encoder.layers.4.mlp.fc1.weight: requires_grad=False\n",
            "encoder.layers.4.mlp.fc1.bias: requires_grad=False\n",
            "encoder.layers.4.mlp.fc2.weight: requires_grad=False\n",
            "encoder.layers.4.mlp.fc2.bias: requires_grad=False\n",
            "encoder.layers.4.layer_norm2.weight: requires_grad=False\n",
            "encoder.layers.4.layer_norm2.bias: requires_grad=False\n",
            "encoder.layers.5.self_attn.qkv.weight: requires_grad=False\n",
            "encoder.layers.5.self_attn.qkv.bias: requires_grad=False\n",
            "encoder.layers.5.self_attn.projection.weight: requires_grad=False\n",
            "encoder.layers.5.self_attn.projection.bias: requires_grad=False\n",
            "encoder.layers.5.layer_norm1.weight: requires_grad=False\n",
            "encoder.layers.5.layer_norm1.bias: requires_grad=False\n",
            "encoder.layers.5.mlp.fc1.weight: requires_grad=False\n",
            "encoder.layers.5.mlp.fc1.bias: requires_grad=False\n",
            "encoder.layers.5.mlp.fc2.weight: requires_grad=False\n",
            "encoder.layers.5.mlp.fc2.bias: requires_grad=False\n",
            "encoder.layers.5.layer_norm2.weight: requires_grad=False\n",
            "encoder.layers.5.layer_norm2.bias: requires_grad=False\n",
            "encoder.layers.6.self_attn.qkv.weight: requires_grad=False\n",
            "encoder.layers.6.self_attn.qkv.bias: requires_grad=False\n",
            "encoder.layers.6.self_attn.projection.weight: requires_grad=False\n",
            "encoder.layers.6.self_attn.projection.bias: requires_grad=False\n",
            "encoder.layers.6.layer_norm1.weight: requires_grad=False\n",
            "encoder.layers.6.layer_norm1.bias: requires_grad=False\n",
            "encoder.layers.6.mlp.fc1.weight: requires_grad=False\n",
            "encoder.layers.6.mlp.fc1.bias: requires_grad=False\n",
            "encoder.layers.6.mlp.fc2.weight: requires_grad=False\n",
            "encoder.layers.6.mlp.fc2.bias: requires_grad=False\n",
            "encoder.layers.6.layer_norm2.weight: requires_grad=False\n",
            "encoder.layers.6.layer_norm2.bias: requires_grad=False\n",
            "encoder.layers.7.self_attn.qkv.weight: requires_grad=False\n",
            "encoder.layers.7.self_attn.qkv.bias: requires_grad=False\n",
            "encoder.layers.7.self_attn.projection.weight: requires_grad=False\n",
            "encoder.layers.7.self_attn.projection.bias: requires_grad=False\n",
            "encoder.layers.7.layer_norm1.weight: requires_grad=False\n",
            "encoder.layers.7.layer_norm1.bias: requires_grad=False\n",
            "encoder.layers.7.mlp.fc1.weight: requires_grad=False\n",
            "encoder.layers.7.mlp.fc1.bias: requires_grad=False\n",
            "encoder.layers.7.mlp.fc2.weight: requires_grad=False\n",
            "encoder.layers.7.mlp.fc2.bias: requires_grad=False\n",
            "encoder.layers.7.layer_norm2.weight: requires_grad=False\n",
            "encoder.layers.7.layer_norm2.bias: requires_grad=False\n",
            "encoder.layers.8.self_attn.qkv.weight: requires_grad=False\n",
            "encoder.layers.8.self_attn.qkv.bias: requires_grad=False\n",
            "encoder.layers.8.self_attn.projection.weight: requires_grad=False\n",
            "encoder.layers.8.self_attn.projection.bias: requires_grad=False\n",
            "encoder.layers.8.layer_norm1.weight: requires_grad=False\n",
            "encoder.layers.8.layer_norm1.bias: requires_grad=False\n",
            "encoder.layers.8.mlp.fc1.weight: requires_grad=False\n",
            "encoder.layers.8.mlp.fc1.bias: requires_grad=False\n",
            "encoder.layers.8.mlp.fc2.weight: requires_grad=False\n",
            "encoder.layers.8.mlp.fc2.bias: requires_grad=False\n",
            "encoder.layers.8.layer_norm2.weight: requires_grad=False\n",
            "encoder.layers.8.layer_norm2.bias: requires_grad=False\n",
            "encoder.layers.9.self_attn.qkv.weight: requires_grad=False\n",
            "encoder.layers.9.self_attn.qkv.bias: requires_grad=False\n",
            "encoder.layers.9.self_attn.projection.weight: requires_grad=False\n",
            "encoder.layers.9.self_attn.projection.bias: requires_grad=False\n",
            "encoder.layers.9.layer_norm1.weight: requires_grad=False\n",
            "encoder.layers.9.layer_norm1.bias: requires_grad=False\n",
            "encoder.layers.9.mlp.fc1.weight: requires_grad=False\n",
            "encoder.layers.9.mlp.fc1.bias: requires_grad=False\n",
            "encoder.layers.9.mlp.fc2.weight: requires_grad=False\n",
            "encoder.layers.9.mlp.fc2.bias: requires_grad=False\n",
            "encoder.layers.9.layer_norm2.weight: requires_grad=False\n",
            "encoder.layers.9.layer_norm2.bias: requires_grad=False\n",
            "encoder.layers.10.self_attn.qkv.weight: requires_grad=False\n",
            "encoder.layers.10.self_attn.qkv.bias: requires_grad=False\n",
            "encoder.layers.10.self_attn.projection.weight: requires_grad=False\n",
            "encoder.layers.10.self_attn.projection.bias: requires_grad=False\n",
            "encoder.layers.10.layer_norm1.weight: requires_grad=False\n",
            "encoder.layers.10.layer_norm1.bias: requires_grad=False\n",
            "encoder.layers.10.mlp.fc1.weight: requires_grad=False\n",
            "encoder.layers.10.mlp.fc1.bias: requires_grad=False\n",
            "encoder.layers.10.mlp.fc2.weight: requires_grad=False\n",
            "encoder.layers.10.mlp.fc2.bias: requires_grad=False\n",
            "encoder.layers.10.layer_norm2.weight: requires_grad=False\n",
            "encoder.layers.10.layer_norm2.bias: requires_grad=False\n",
            "encoder.layers.11.self_attn.qkv.weight: requires_grad=False\n",
            "encoder.layers.11.self_attn.qkv.bias: requires_grad=False\n",
            "encoder.layers.11.self_attn.projection.weight: requires_grad=False\n",
            "encoder.layers.11.self_attn.projection.bias: requires_grad=False\n",
            "encoder.layers.11.layer_norm1.weight: requires_grad=False\n",
            "encoder.layers.11.layer_norm1.bias: requires_grad=False\n",
            "encoder.layers.11.mlp.fc1.weight: requires_grad=False\n",
            "encoder.layers.11.mlp.fc1.bias: requires_grad=False\n",
            "encoder.layers.11.mlp.fc2.weight: requires_grad=False\n",
            "encoder.layers.11.mlp.fc2.bias: requires_grad=False\n",
            "encoder.layers.11.layer_norm2.weight: requires_grad=False\n",
            "encoder.layers.11.layer_norm2.bias: requires_grad=False\n",
            "post_layernorm.weight: requires_grad=False\n",
            "post_layernorm.bias: requires_grad=False\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BlipForConditionalGeneration(\n",
              "  (vision_model): BlipVisionModel(\n",
              "    (embeddings): BlipVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "    )\n",
              "    (encoder): BlipEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x BlipEncoderLayer(\n",
              "          (self_attn): BlipAttention(\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): BlipMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (text_decoder): BlipTextLMHeadModel(\n",
              "    (bert): BlipTextModel(\n",
              "      (embeddings): BlipTextEmbeddings(\n",
              "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (encoder): BlipTextEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x BlipTextLayer(\n",
              "            (attention): BlipTextAttention(\n",
              "              (self): BlipTextSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (output): BlipTextSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BlipTextAttention(\n",
              "              (self): BlipTextSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (output): BlipTextSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BlipTextIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BlipTextOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (cls): BlipTextOnlyMLMHead(\n",
              "      (predictions): BlipTextLMPredictionHead(\n",
              "        (transform): BlipTextPredictionHeadTransform(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (transform_act_fn): GELUActivation()\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "check = []\n",
        "for cp in dataset['train']['text']:\n",
        "  check.append(len(cp))\n",
        "\n",
        "plt.hist(check, bins = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "tB5WdNdaQ39n",
        "outputId": "8b4d153d-bc9d-43eb-a9af-35d66da4f6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 285., 1042., 1949., 2292., 2256., 1939., 1560.,  891.,  206.,\n",
              "          17.]),\n",
              " array([ 60. ,  83.4, 106.8, 130.2, 153.6, 177. , 200.4, 223.8, 247.2,\n",
              "        270.6, 294. ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max(check)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcI3N6EySbFS",
        "outputId": "cc58695b-934f-4673-99f3-8618b7500b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "294"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    # Process images\n",
        "    images = []\n",
        "    for img in examples['image']:\n",
        "        images.append(img.convert(\"RGB\"))\n",
        "\n",
        "    # Tokenize captions\n",
        "    inputs = processor(images=images, text=examples['text'],\n",
        "                      max_length=MAX_LENGTH, padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # Assign labels (copy of input_ids)\n",
        "    inputs[\"labels\"] = inputs[\"input_ids\"].copy()\n",
        "\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "NyvurdI5MCcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset = dataset['train'].train_test_split(test_size=TEST_SPLIT_PERCENTAGE / 100, seed=42)\n",
        "print(\"\\nSplit Dataset Structure:\")\n",
        "print(split_dataset)\n",
        "\n",
        "# Assign splits\n",
        "train_dataset = split_dataset['train']\n",
        "test_dataset = split_dataset['test']\n",
        "\n",
        "# -------------------------------\n",
        "# Apply Preprocessing to Dataset\n",
        "# -------------------------------\n",
        "print(\"\\nPreprocessing the training data...\")\n",
        "tokenized_train = train_dataset.map(preprocess_function, batched=True, batch_size = BATCH_SIZE, remove_columns=dataset['train'].column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "399fe1e2b7e94d32bb53feb33122974c",
            "ba5ed68d4cc54a24b36820770c1d8033",
            "914ff72736ac419094d3301f40db98f1",
            "d9b9a8956fcc47aeaabd539a3f4ac420",
            "0782e48104614decaf3a4ba31a4ed1c2",
            "bd5a85a709204ddca8dc6179321e0de6",
            "81ac2128a3ee448882c4988ec2e60175",
            "307c96240fca426aae05fbe653891d4d",
            "2e20d06086ca4f7bad53a1dbc6c80130",
            "29a9882e9de0445592d27e7510c7d5d4",
            "00a4c9276a434adfab025cb3014346cc"
          ]
        },
        "id": "EKkMPnfHUpnd",
        "outputId": "bd8a8dbb-ded2-4564-86d8-c0a98a3e0e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Split Dataset Structure:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'image'],\n",
            "        num_rows: 9949\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'image'],\n",
            "        num_rows: 2488\n",
            "    })\n",
            "})\n",
            "\n",
            "Preprocessing the training data...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/9949 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "399fe1e2b7e94d32bb53feb33122974c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(processor.tokenizer, model=model)"
      ],
      "metadata": {
        "id": "IhqdayH-ihit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,  # Enable mixed precision if using GPU\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    seed=SEED,\n",
        "    gradient_accumulation_steps=8\n",
        ")"
      ],
      "metadata": {
        "id": "MTYitIb9k63B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76075b68-8ef8-4fcb-a59b-3d9acac66043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    # If you have a validation set, you can pass it here using eval_dataset\n",
        "    # eval_dataset=tokenized_validation,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    # compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Fine-Tune the Model\n",
        "# -------------------------------\n",
        "print(\"\\nStarting fine-tuning...\")\n",
        "trainer.train()\n",
        "\n",
        "# -------------------------------\n",
        "# Save the Fine-Tuned Model\n",
        "# -------------------------------\n",
        "print(f\"\\nSaving the fine-tuned model to {OUTPUT_DIR}...\")\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "processor.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "id": "QpGJ5tb6x2-6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "outputId": "b06b09d3-1be9-40af-f49b-79b3a6720e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='37' max='465' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 37/465 13:22 < 2:43:27, 0.04 it/s, Epoch 0.23/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x78f9f29a3490>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n",
            "Exception ignored in: <function _xla_gc_callback at 0x78f9f29a3490>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n",
            "Exception ignored in: <function _xla_gc_callback at 0x78f9f29a3490>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-fbd52137a6ab>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# -------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting fine-tuning...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# -------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1939\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2237\u001b[0m                 \u001b[0mtotal_batched_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m                     \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m                 \u001b[0mnext_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mcurrent_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;31m# run through tokenizer without labels to ensure no side effects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         batch = pad_without_fast_tokenizer_warning(\n\u001b[0m\u001b[1;32m    599\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mnon_labels_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpad_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpad_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Restore the state of the warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3558\u001b[0m                 \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3560\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3562\u001b[0m     def create_token_type_ids_from_sequences(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0;31m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mas_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtensor_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJAX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define evaluation metrics\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "meteor = evaluate.load(\"meteor\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "cider = evaluate.load(\"cider\")\n",
        "\n",
        "# -------------------------------\n",
        "# Define Compute Metrics Function\n",
        "# -------------------------------\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = processor.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
        "    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # References should be a list of list of references\n",
        "    references = [[label] for label in decoded_labels]\n",
        "    predictions = decoded_preds\n",
        "\n",
        "    # Compute metrics\n",
        "    bleu_result = bleu.compute(predictions=predictions, references=references)\n",
        "    meteor_result = meteor.compute(predictions=predictions, references=references)\n",
        "    rouge_result = rouge.compute(predictions=predictions, references=references)\n",
        "    cider_result = cider.compute(predictions=predictions, references=references)\n",
        "\n",
        "    return {\n",
        "        \"bleu\": bleu_result[\"bleu\"],\n",
        "        \"meteor\": meteor_result[\"meteor\"],\n",
        "        \"rouge1\": rouge_result[\"rouge1\"],\n",
        "        \"rouge2\": rouge_result[\"rouge2\"],\n",
        "        \"rougeL\": rouge_result[\"rougeL\"],\n",
        "        \"cider\": cider_result[\"cider\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "qmofoHgs1Vwl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}