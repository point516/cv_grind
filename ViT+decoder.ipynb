{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBQeUhDXzAPf",
        "outputId": "cdd05009-45fb-43ce-ebcb-e62dbc9ea194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNOZquB8nfMT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import ViTModel, ViTConfig, BertTokenizer, ViTImageProcessor\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "from datasets import DatasetDict\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCjarPcpnkID",
        "outputId": "b5548d68-146d-491f-c48b-390b6139f51e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"tomytjandra/h-and-m-fashion-caption-12k\")  # Replace with your dataset path or identifier\n",
        "\n",
        "# Initialize the tokenizer (you can choose a different tokenizer if preferred)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define image transformations\n",
        "# image_transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),  # ViT typically expects 224x224 images\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet statistics\n",
        "#                          std=[0.229, 0.224, 0.225]),\n",
        "# ])\n",
        "\n",
        "processor = ViTImageProcessor().from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    # Process images\n",
        "    images = [image.convert(\"RGB\") for image in examples['image']]\n",
        "    encoding = processor(images=images)\n",
        "    examples['pixel_values'] = encoding['pixel_values']\n",
        "\n",
        "    # Tokenize captions\n",
        "    captions = examples['text']\n",
        "    encoding = tokenizer(captions, padding='max_length', truncation=True, max_length=224)\n",
        "    examples['input_ids'] = encoding['input_ids']\n",
        "    examples['attention_mask'] = encoding['attention_mask']\n",
        "\n",
        "    return examples\n",
        "\n",
        "# Optional: Verify the splits\n",
        "processed_ds = dataset.map(preprocess_function, batched=True, batch_size=100, remove_columns=['text', 'image'])\n",
        "\n",
        "processed_ds.set_format(\n",
        "    type='torch',\n",
        "    columns=['pixel_values', 'input_ids', 'attention_mask']\n",
        ")\n",
        "\n",
        "# Step 1: Split into train_val and test\n",
        "train_val_split = processed_ds['train'].train_test_split(test_size=1250, seed=42)  # 10% for test\n",
        "train_val = train_val_split['train']\n",
        "test = train_val_split['test']\n",
        "\n",
        "# Step 2: Split train_val into train and validation\n",
        "train_validation_split = train_val.train_test_split(test_size=1250, seed=42)\n",
        "train = train_validation_split['train']\n",
        "validation = train_validation_split['test']\n",
        "\n",
        "# Step 3: Create a new DatasetDict with the splits\n",
        "new_dataset = DatasetDict({\n",
        "    'train': train,\n",
        "    'validation': validation,\n",
        "    'test': test\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK3oFHETvUi6"
      },
      "outputs": [],
      "source": [
        "# Define training parameters\n",
        "batch_size = 32\n",
        "\n",
        "# Create DataLoader for training\n",
        "train_loader = DataLoader(new_dataset['train'], batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(new_dataset['validation'], batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubBnQIbTvMTt"
      },
      "outputs": [],
      "source": [
        "class ImageCaptioningModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, decoder_layers, decoder_heads, decoder_ffn_dim, max_seq_length=224):\n",
        "        super(ImageCaptioningModel, self).__init__() #initialize from parent .init()\n",
        "\n",
        "        # Encoder: Vision Transformer (ViT)\n",
        "        vit_config = ViTConfig.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        vit_config.num_hidden_layers = 6  # Reduce the number of layers to 4-6\n",
        "        self.encoder = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k', config=vit_config)\n",
        "\n",
        "        # Decoder: Transformer\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=decoder_heads, dim_feedforward=decoder_ffn_dim)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=decoder_layers)\n",
        "\n",
        "        # Embedding for input tokens\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, embed_dim)) # ??????????????\n",
        "\n",
        "        # Final linear layer to generate vocabulary scores\n",
        "        self.output_linear = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        # Projection to match dimensions\n",
        "        self.encoder_proj = nn.Linear(vit_config.hidden_size, embed_dim)\n",
        "\n",
        "    def forward(self, pixel_values, input_ids, attention_mask):\n",
        "        # Encoder\n",
        "        encoder_outputs = self.encoder(pixel_values=pixel_values)\n",
        "        encoder_hidden_states = encoder_outputs.last_hidden_state  # (batch_size, num_patches + 1, hidden_size)\n",
        "        # Project encoder outputs to embed_dim\n",
        "        encoder_proj = self.encoder_proj(encoder_hidden_states)  # (batch_size, seq_len, embed_dim)\n",
        "        encoder_proj = encoder_proj.permute(1, 0, 2)  # (seq_len, batch_size, embed_dim)\n",
        "\n",
        "        # Decoder\n",
        "        embeddings = self.token_embedding(input_ids) + self.positional_encoding[:, :input_ids.size(1), :]\n",
        "        embeddings = embeddings.permute(1, 0, 2)  # (seq_len, batch_size, embed_dim)\n",
        "\n",
        "\n",
        "        decoder_outputs = self.decoder(embeddings, encoder_proj, tgt_key_padding_mask=~attention_mask.bool())\n",
        "        decoder_outputs = decoder_outputs.permute(1, 0, 2)  # (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        outputs = self.output_linear(decoder_outputs)  # (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNGJJoH_kQlj",
        "outputId": "79373bd4-929d-4529-d3ba-3dba5fcfd748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.7.layernorm_after.weight', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Define vocabulary size and other hyperparameters\n",
        "vocab_size = tokenizer.vocab_size\n",
        "embed_dim = 512\n",
        "decoder_layers = 6  # 4-6 layers as per requirement\n",
        "decoder_heads = 8\n",
        "decoder_ffn_dim = 2048\n",
        "max_seq_length = 224\n",
        "\n",
        "# Initialize the model\n",
        "model = ImageCaptioningModel(vocab_size, embed_dim, decoder_layers, decoder_heads, decoder_ffn_dim, max_seq_length)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "# Define separate learning rates\n",
        "learning_rate_encoder = 1e-5  # Lower learning rate for pre-trained encoder\n",
        "learning_rate_decoder = 1e-4  # Higher learning rate for decoder\n",
        "# Create parameter groups\n",
        "optimizer = optim.AdamW([\n",
        "    {'params': model.encoder.parameters(), 'lr': learning_rate_encoder},\n",
        "    {'params': model.decoder.parameters(), 'lr': learning_rate_decoder},\n",
        "    {'params': model.token_embedding.parameters(), 'lr': learning_rate_decoder},\n",
        "    {'params': model.encoder_proj.parameters(), 'lr': learning_rate_decoder},\n",
        "    {'params': model.output_linear.parameters(), 'lr': learning_rate_decoder}\n",
        "], betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Initialize the scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                 factor=0.5, patience=2,\n",
        "                                                 verbose=True, min_lr=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlzW2LyAQ8DI",
        "outputId": "797e1d75-7549-4b15-eb80-68a844b6ebb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 3.1553, val_loss: 1.5552, lr: [1e-05, 0.0001, 0.0001, 0.0001, 0.0001]\n",
            "Epoch 2/10, Loss: 1.3076, val_loss: 1.1033, lr: [1e-05, 0.0001, 0.0001, 0.0001, 0.0001]\n",
            "Epoch 3/10, Loss: 0.9947, val_loss: 0.9175, lr: [1e-05, 0.0001, 0.0001, 0.0001, 0.0001]\n",
            "Epoch 4/10, Loss: 0.8530, val_loss: 0.8573, lr: [1e-05, 0.0001, 0.0001, 0.0001, 0.0001]\n",
            "Epoch 5/10, Loss: 0.7621, val_loss: 0.7923, lr: [1e-05, 0.0001, 0.0001, 0.0001, 0.0001]\n",
            "Epoch 6/10, Loss: 0.6940, val_loss: 0.7607, lr: [1e-05, 0.0001, 0.0001, 0.0001, 0.0001]\n",
            "Epoch 7/10, Loss: 0.6404, val_loss: 0.7328, lr: [1e-05, 0.0001, 0.0001, 0.0001, 0.0001]\n",
            "Epoch 8/10, Loss: 0.5951, val_loss: 0.7264, lr: [1e-05, 0.0001, 0.0001, 0.0001, 0.0001]\n",
            "Epoch 9/10, Loss: 0.5548, val_loss: 0.7108, lr: [1e-05, 0.0001, 0.0001, 0.0001, 0.0001]\n",
            "Epoch 10/10, Loss: 0.5187, val_loss: 0.7305, lr: [1e-05, 0.0001, 0.0001, 0.0001, 0.0001]\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_val_loss = 0\n",
        "    for batch in train_loader:\n",
        "        pixel_values = batch['pixel_values'].to(device)  # (batch_size, 3, 224, 224)\n",
        "        input_ids = batch['input_ids'].to(device)        # (batch_size, seq_length)\n",
        "        attention_mask = batch['attention_mask'].to(device)  # (batch_size, seq_length)\n",
        "\n",
        "        # Shift input_ids and create labels\n",
        "        # Typically, input_ids are shifted right for the decoder input\n",
        "        # Labels are the actual tokens to predict\n",
        "        labels = input_ids[:, 1:].contiguous()\n",
        "        decoder_input_ids = input_ids[:, :-1].contiguous()\n",
        "        decoder_attention_mask = attention_mask[:, :-1].contiguous()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(pixel_values, decoder_input_ids, decoder_attention_mask)\n",
        "        # outputs: (batch_size, seq_length -1, vocab_size)\n",
        "\n",
        "        loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            pixel_values = batch['pixel_values'].to(device)  # (batch_size, 3, 224, 224)\n",
        "            input_ids = batch['input_ids'].to(device)        # (batch_size, seq_length)\n",
        "            attention_mask = batch['attention_mask'].to(device)  # (batch_size, seq_length)\n",
        "\n",
        "            labels = input_ids[:, 1:].contiguous()\n",
        "            decoder_input_ids = input_ids[:, :-1].contiguous()\n",
        "            decoder_attention_mask = attention_mask[:, :-1].contiguous()\n",
        "\n",
        "            outputs = model(pixel_values, decoder_input_ids, decoder_attention_mask)\n",
        "\n",
        "            loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n",
        "\n",
        "            epoch_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, val_loss: {avg_val_loss:.4f}, lr: {scheduler.get_last_lr()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EjEuRcH953k"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/VIT+decoder/VIT_decoder.pth')\n",
        "\n",
        "# model = MyModel()\n",
        "# model.load_state_dict(torch.load('model_state.pth'))\n",
        "# model.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4und3_A-bir"
      },
      "outputs": [],
      "source": [
        "#inference\n",
        "#tgt_mask nad what it mean to attent only to previous tokens (look from MLCV lectures)\n",
        "#save on each epoch or each N epochs\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pbMcOyp3F-7m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}