{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5ada9eb38af14a09b93f143437df5d90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be462a4441664fecafd6722fe6cb3e01",
              "IPY_MODEL_08a229f46f264dc49106b28fd28b1b01",
              "IPY_MODEL_5f8f62f1a04943f98f605c5b27a9a916"
            ],
            "layout": "IPY_MODEL_ef501d56e6ed4ebc98109e4189b99a60"
          }
        },
        "be462a4441664fecafd6722fe6cb3e01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9b29d434e4445a2a1d306697840872c",
            "placeholder": "​",
            "style": "IPY_MODEL_f1572b491d924ffca7be9949680eb362",
            "value": "Map: 100%"
          }
        },
        "08a229f46f264dc49106b28fd28b1b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf26be1dfafa4e4fbce4d23803c138bd",
            "max": 9949,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cdff33ad2f8d46b0a006512d75e815db",
            "value": 9949
          }
        },
        "5f8f62f1a04943f98f605c5b27a9a916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95b6a2f70607421b96de3a0fa8a4f64d",
            "placeholder": "​",
            "style": "IPY_MODEL_ec82983438794198b2134392dba84498",
            "value": " 9949/9949 [07:29&lt;00:00, 25.04 examples/s]"
          }
        },
        "ef501d56e6ed4ebc98109e4189b99a60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9b29d434e4445a2a1d306697840872c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1572b491d924ffca7be9949680eb362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf26be1dfafa4e4fbce4d23803c138bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdff33ad2f8d46b0a006512d75e815db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95b6a2f70607421b96de3a0fa8a4f64d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec82983438794198b2134392dba84498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBQeUhDXzAPf",
        "outputId": "fc548c2b-564f-41fa-8ac3-16ef3ecaec5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import ViTModel, ViTConfig, BertTokenizer\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "aNOZquB8nfMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"tomytjandra/h-and-m-fashion-caption-12k\")  # Replace with your dataset path or identifier\n",
        "\n",
        "# Initialize the tokenizer (you can choose a different tokenizer if preferred)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define image transformations\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # ViT typically expects 224x224 images\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet statistics\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    # Process images\n",
        "    images = [image_transform(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    examples['pixel_values'] = images\n",
        "\n",
        "    # Tokenize captions\n",
        "    captions = examples['text']\n",
        "    encoding = tokenizer(captions, padding='max_length', truncation=True, max_length=224, return_tensors=\"pt\")\n",
        "    examples['input_ids'] = encoding['input_ids']\n",
        "    examples['attention_mask'] = encoding['attention_mask']\n",
        "\n",
        "    return examples\n",
        "\n",
        "# Step 1: Split into train_val and test\n",
        "train_val_split = dataset['train'].train_test_split(test_size=1250, seed=42)  # 10% for test\n",
        "train_val = train_val_split['train']\n",
        "test = train_val_split['test']\n",
        "\n",
        "# Step 2: Split train_val into train and validation\n",
        "train_validation_split = train_val.train_test_split(test_size=1250, seed=42)\n",
        "train = train_validation_split['train']\n",
        "validation = train_validation_split['test']\n",
        "\n",
        "# Step 3: Create a new DatasetDict with the splits\n",
        "new_dataset = DatasetDict({\n",
        "    'train': train,\n",
        "    'validation': validation,\n",
        "    'test': test\n",
        "})\n",
        "\n",
        "# Optional: Verify the splits\n",
        "print(new_dataset)\n",
        "processed_train = new_dataset['train'].map(preprocess_function, batched=True, batch_size=100, remove_columns=['text', 'image'])\n",
        "processed_val = new_dataset['validation'].map(preprocess_function, batched=True, batch_size=100, remove_columns=['text', 'image'])\n",
        "processed_test = new_dataset['test'].map(preprocess_function, batched=True, batch_size=100, remove_columns=['text', 'image'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416,
          "referenced_widgets": [
            "5ada9eb38af14a09b93f143437df5d90",
            "be462a4441664fecafd6722fe6cb3e01",
            "08a229f46f264dc49106b28fd28b1b01",
            "5f8f62f1a04943f98f605c5b27a9a916",
            "ef501d56e6ed4ebc98109e4189b99a60",
            "b9b29d434e4445a2a1d306697840872c",
            "f1572b491d924ffca7be9949680eb362",
            "cf26be1dfafa4e4fbce4d23803c138bd",
            "cdff33ad2f8d46b0a006512d75e815db",
            "95b6a2f70607421b96de3a0fa8a4f64d",
            "ec82983438794198b2134392dba84498"
          ]
        },
        "id": "yCjarPcpnkID",
        "outputId": "65b7d9ac-e85e-4de3-81dc-1993a7c5eedd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Split Dataset Structure:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'image'],\n",
            "        num_rows: 9949\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'image'],\n",
            "        num_rows: 2488\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/9949 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ada9eb38af14a09b93f143437df5d90"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training parameters\n",
        "batch_size = 32\n",
        "\n",
        "# Create DataLoader for training\n",
        "train_loader = DataLoader(processed_train, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(processed_val, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "MK3oFHETvUi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptioningModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, decoder_layers, decoder_heads, decoder_ffn_dim, max_seq_length=224):\n",
        "        super(ImageCaptioningModel, self).__init__() #initialize from parent .init()\n",
        "\n",
        "        # Encoder: Vision Transformer (ViT)\n",
        "        vit_config = ViTConfig.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        vit_config.num_hidden_layers = 6  # Reduce the number of layers to 4-6\n",
        "        self.encoder = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k', config=vit_config)\n",
        "\n",
        "        # Decoder: Transformer\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=vit_config.hidden_size, nhead=decoder_heads, dim_feedforward=decoder_ffn_dim)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=decoder_layers)\n",
        "\n",
        "        # Embedding for input tokens\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, embed_dim)) # ??????????????\n",
        "\n",
        "        # Final linear layer to generate vocabulary scores\n",
        "        self.output_linear = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        # Projection to match dimensions\n",
        "        self.encoder_proj = nn.Linear(vit_config.hidden_size, embed_dim)\n",
        "        self.decoder_proj = nn.Linear(embed_dim, vit_config.hidden_size)\n",
        "\n",
        "    def forward(self, pixel_values, input_ids, attention_mask):\n",
        "        # Encoder\n",
        "        encoder_outputs = self.encoder(pixel_values=pixel_values)\n",
        "        encoder_hidden_states = encoder_outputs.last_hidden_state  # (batch_size, num_patches + 1, hidden_size)\n",
        "        # Project encoder outputs to embed_dim\n",
        "        encoder_proj = self.encoder_proj(encoder_hidden_states)  # (batch_size, seq_len, embed_dim)\n",
        "        encoder_proj = encoder_proj.permute(1, 0, 2)  # (seq_len, batch_size, embed_dim)\n",
        "\n",
        "        # Decoder\n",
        "        embeddings = self.token_embedding(input_ids) + self.positional_encoding[:, :input_ids.size(1), :]\n",
        "        embeddings = embeddings.permute(1, 0, 2)  # (seq_len, batch_size, embed_dim)\n",
        "\n",
        "        decoder_outputs = self.decoder(embeddings, encoder_proj, tgt_key_padding_mask=~attention_mask.bool())\n",
        "        decoder_outputs = decoder_outputs.permute(1, 0, 2)  # (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        outputs = self.output_linear(decoder_outputs)  # (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "ubBnQIbTvMTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define vocabulary size and other hyperparameters\n",
        "vocab_size = tokenizer.vocab_size\n",
        "embed_dim = 512\n",
        "decoder_layers = 6  # 4-6 layers as per requirement\n",
        "decoder_heads = 8\n",
        "decoder_ffn_dim = 2048\n",
        "max_seq_length = 30\n",
        "\n",
        "# Initialize the model\n",
        "model = ImageCaptioningModel(vocab_size, embed_dim, decoder_layers, decoder_heads, decoder_ffn_dim, max_seq_length)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "# Define separate learning rates\n",
        "learning_rate_encoder = 1e-5  # Lower learning rate for pre-trained encoder\n",
        "learning_rate_decoder = 1e-4  # Higher learning rate for decoder\n",
        "# Create parameter groups\n",
        "optimizer = optim.AdamW([\n",
        "    {'params': model.encoder.parameters(), 'lr': learning_rate_encoder},\n",
        "    {'params': model.decoder.parameters(), 'lr': learning_rate_decoder},\n",
        "    {'params': model.token_embedding.parameters(), 'lr': learning_rate_decoder},\n",
        "    {'params': model.encoder_proj.parameters(), 'lr': learning_rate_decoder},\n",
        "    {'params': model.decoder_proj.parameters(), 'lr': learning_rate_decoder},\n",
        "    {'params': model.output_linear.parameters(), 'lr': learning_rate_decoder}\n",
        "], betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Initialize the scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                 factor=0.5, patience=2,\n",
        "                                                 verbose=True, min_lr=1e-6)"
      ],
      "metadata": {
        "id": "ZNGJJoH_kQlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_val_loss = 0\n",
        "    for batch in train_loader:\n",
        "        pixel_values = batch['pixel_values'].to(device)  # (batch_size, 3, 224, 224)\n",
        "        input_ids = batch['input_ids'].to(device)        # (batch_size, seq_length)\n",
        "        attention_mask = batch['attention_mask'].to(device)  # (batch_size, seq_length)\n",
        "\n",
        "        # Shift input_ids and create labels\n",
        "        # Typically, input_ids are shifted right for the decoder input\n",
        "        # Labels are the actual tokens to predict\n",
        "        labels = input_ids[:, 1:].contiguous()\n",
        "        decoder_input_ids = input_ids[:, :-1].contiguous()\n",
        "        decoder_attention_mask = attention_mask[:, :-1].contiguous()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(pixel_values, decoder_input_ids, decoder_attention_mask)\n",
        "        # outputs: (batch_size, seq_length -1, vocab_size)\n",
        "\n",
        "        loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            pixel_values = batch['pixel_values'].to(device)  # (batch_size, 3, 224, 224)\n",
        "            input_ids = batch['input_ids'].to(device)        # (batch_size, seq_length)\n",
        "            attention_mask = batch['attention_mask'].to(device)  # (batch_size, seq_length)\n",
        "\n",
        "            labels = input_ids[:, 1:].contiguous()\n",
        "            decoder_input_ids = input_ids[:, :-1].contiguous()\n",
        "            decoder_attention_mask = attention_mask[:, :-1].contiguous()\n",
        "\n",
        "            outputs = model(pixel_values, decoder_input_ids, decoder_attention_mask)\n",
        "\n",
        "            loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n",
        "\n",
        "            epoch_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, val_loss: {avg_val_loss:.4f}, lr: {scheduler.get_last_lr()}\")\n"
      ],
      "metadata": {
        "id": "PlzW2LyAQ8DI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}